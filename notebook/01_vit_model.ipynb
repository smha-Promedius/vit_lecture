{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDpj1WTRGVVPdOhFfVbapF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smha-Promedius/vit_lecture/blob/master/notebook/01_vit_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-m24d2u5IXij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff2965ba-4138-4a23-e057-1625ee80ff45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 4.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 31 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install -y fonts-nanum* | tail -n 1\n",
        "!sudo fc-cache -fv\n",
        "!rm -rf ~/.cache/matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 라이브러리 & 커맨드 준비"
      ],
      "metadata": {
        "id": "jUcViRGPCOcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요 라이브러리 설치\n",
        "\n",
        "!pip install torchviz | tail -n 1\n",
        "!pip install torchinfo | tail -n 1\n",
        "w = !apt install tree\n",
        "print(w[-2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZsW0WoUCQr-",
        "outputId": "922d3ed8-06bf-412c-e4f8-44bc84d77325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed torchviz-0.0.2\n",
            "Successfully installed torchinfo-1.7.1\n",
            "Setting up tree (1.7.0-5) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 모든 설치가 끝나면 한글 폰트를 바르게 출력하기 위해 **[런타임]** -> **[런타임 다시시작]**을 클릭한 다음, 아래 셀부터 코드를 실행해 주십시오."
      ],
      "metadata": {
        "id": "9MpRCzr8CTHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 임포트\n",
        "\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "\n",
        "# 폰트 관련 용도\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "# 나눔 고딕 폰트의 경로 명시\n",
        "path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
        "font_name = fm.FontProperties(fname=path, size=10).get_name()\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchinfo import summary\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "ZrpR2QeMCTtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 초기설정"
      ],
      "metadata": {
        "id": "g8KviGvXdSrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# warning 표시 끄기\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "# 기본 폰트 설정\n",
        "plt.rcParams['font.family'] = font_name\n",
        "\n",
        "# 기본 폰트 사이즈 변경\n",
        "plt.rcParams['font.size'] = 14\n",
        "\n",
        "# 기본 그래프 사이즈 변경\n",
        "plt.rcParams['figure.figsize'] = (6,6)\n",
        "\n",
        "# 기본 그리드 표시\n",
        "# 필요에 따라 설정할 때는, plt.grid()\n",
        "plt.rcParams['axes.grid'] = True\n",
        "\n",
        "# 마이너스 기호 정상 출력\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 넘파이 부동소수점 자릿수 표시\n",
        "np.set_printoptions(suppress=True, precision=4)"
      ],
      "metadata": {
        "id": "kC29AxQGdROP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU 디바이스 할당\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdnx-HuTCyKh",
        "outputId": "325cd72b-8ec7-4f03-e7e1-d0b8dadf197b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 구축"
      ],
      "metadata": {
        "id": "0k0UDyAOf9w3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VitInputLayer(nn.Module): \n",
        "    def __init__(self, in_channels:int=3, emb_dim:int=384, num_patch_row:int=4, image_size:int=224):\n",
        "        \"\"\" \n",
        "        인수 : \n",
        "            in_channels: 입력 이미지 채널 수\n",
        "            emb_dim: 임베딩 벡터 길이\n",
        "            num_patch_row: 높이 방향 패치 수. 예시는 2x2이므로 2를 기본값으로 함 \n",
        "            image_size: 입력 이미지 한 변의 길이. 입력 이미지의 높이와 폭은 동일하다고 가정\n",
        "        \"\"\"\n",
        "        super(VitInputLayer, self).__init__() \n",
        "        self.in_channels=in_channels \n",
        "        self.emb_dim = emb_dim \n",
        "        self.num_patch_row = num_patch_row \n",
        "        self.image_size = image_size\n",
        "        \n",
        "        # 패치 수\n",
        "        ## 예: 입력 이미지를 2x2 패치로 나눴을 경우, num_patch는 4\n",
        "        self.num_patch = self.num_patch_row**2\n",
        "\n",
        "        # 패치 크기\n",
        "        ## 예: 입력 이미지 한 변의 길이가 32인 경우, patch_size는 16 \n",
        "        self.patch_size = int(self.image_size // self.num_patch_row)\n",
        "\n",
        "        # 입력 이미지를 패치로 분할 & 패치 임베딩을 한번에 수행 \n",
        "        self.patch_emb_layer = nn.Conv2d(\n",
        "            in_channels=self.in_channels, \n",
        "            out_channels=self.emb_dim, \n",
        "            kernel_size=self.patch_size, \n",
        "            stride=self.patch_size\n",
        "        )\n",
        "\n",
        "        # CLS 토큰 \n",
        "        self.cls_token = nn.Parameter(\n",
        "            torch.randn(1, 1, emb_dim) \n",
        "        )\n",
        "\n",
        "        # 위치 임베딩\n",
        "        ## CLS 토큰이 앞에 결속되어 있기 때문에\n",
        "        ## 길이 emb_dim의 위치 임베딩 벡터를 (패치 수 +1)개 준비 \n",
        "        self.pos_emb = nn.Parameter(\n",
        "            torch.randn(1, self.num_patch+1, emb_dim) \n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\" \n",
        "        인수:\n",
        "            x: 입력 이미지. 사이즈는 (B, C, H, W)\n",
        "                B: 배치 사이즈, C: 채널 수, H: 높이, W: 폭\n",
        "        반환값:\n",
        "            z_0: ViT로의 입력값. 사이즈는 (B, N, D)\n",
        "                B: 배치 사이즈, N: 토큰 수, D: 임베딩 벡터 길이\n",
        "        \"\"\"\n",
        "        # 패치 임베딩 & flatten\n",
        "        ## 패치 임베딩 (B, C, H, W) -> (B, D, H/P, W/P) \n",
        "        ## 여기서 P는 패치 한 변의 길이\n",
        "        z_0 = self.patch_emb_layer(x)\n",
        "\n",
        "        ## 패치 flatten (B, D, H/P, W/P) -> (B, D, Np) \n",
        "        ## 여기서 Np는 패치 수(=H*W/Pˆ2)\n",
        "        z_0 = z_0.flatten(2)\n",
        "\n",
        "        ## axis 교환 (B, D, Np) -> (B, Np, D) \n",
        "        z_0 = z_0.transpose(1, 2)\n",
        "\n",
        "        # 패치 임베딩 앞쪽에 CLS 토큰을 결합 \n",
        "        ## (B, Np, D) -> (B, N, D)\n",
        "        ## N = (Np + 1)\n",
        "        ## cls_token의 사이즈는 (1,1,D) 이므로\n",
        "        ## repeat 메서드가 (B,1,D)로 변환하고 나서 패치 임베딩과 결합 \n",
        "        z_0 = torch.cat(\n",
        "            [self.cls_token.repeat(repeats=(x.size(0),1,1)), z_0], dim=1)\n",
        "\n",
        "        # 위치 임베딩을 더함 \n",
        "        ## (B, N, D) -> (B, N, D) \n",
        "        z_0 = z_0 + self.pos_emb\n",
        "        return z_0"
      ],
      "metadata": {
        "id": "iAm_Y8_1C2Oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, channel, height, width= 4, 3, 224, 224\n",
        "x = torch.randn(batch_size, channel, height, width) \n",
        "input_layer = VitInputLayer(num_patch_row=4) \n",
        "z_0=input_layer(x)\n",
        "\n",
        "# (4, 16+1, 384)(=(B, N, D))로 나왔는지 확인 \n",
        "print(z_0.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UCIbMxSE858",
        "outputId": "da3fe845-c460-4134-9dd3-d390b9947e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 17, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module): \n",
        "    def __init__(self, emb_dim:int=384, head:int=3, dropout:float=0.):\n",
        "        \"\"\" \n",
        "        인수:\n",
        "            emb_dim: 임베딩 벡터 길이 \n",
        "            head: 헤드 수\n",
        "            dropout: 드롭 아웃 확률\n",
        "        \"\"\"\n",
        "        super(MultiHeadSelfAttention, self).__init__() \n",
        "        self.head = head\n",
        "        self.emb_dim = emb_dim\n",
        "        self.head_dim = emb_dim // head\n",
        "        self.sqrt_dh = self.head_dim**0.5 # D_h의 제곱근. qk^T를 나누기 위한 계수\n",
        "\n",
        "        # 입력을 q, k, v로 임베딩 하기 위한 선형층 \n",
        "        self.w_q = nn.Linear(emb_dim, emb_dim, bias=False) \n",
        "        self.w_k = nn.Linear(emb_dim, emb_dim, bias=False) \n",
        "        self.w_v = nn.Linear(emb_dim, emb_dim, bias=False)\n",
        "\n",
        "        # 드롭 아웃\n",
        "        self.attn_drop = nn.Dropout(dropout)\n",
        "\n",
        "        # MHSA 결과를 출력에 임베딩 하기 위한 선형층\n",
        "        ## 식에는 없지만 드롭 아웃을 사용함 \n",
        "        self.w_o = nn.Sequential(\n",
        "            nn.Linear(emb_dim, emb_dim),\n",
        "            nn.Dropout(dropout) \n",
        "        )\n",
        "\n",
        "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\" \n",
        "        인수:\n",
        "            z: MHSA로의 입력. 사이즈는 (B, N, D)\n",
        "                B: 배치 사이즈, N: 토큰 수, D: 벡터 길이\n",
        "        반환값:\n",
        "            out: MHSA 출력. 사이즈는 (B, N, D)\n",
        "                B: 배치 사이즈, N: 토큰 수, D: 임베딩 벡터 길이\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, num_patch, _ = z.size()\n",
        "\n",
        "        # 임베딩\n",
        "        ## (B, N, D) -> (B, N, D)\n",
        "        q = self.w_q(z)\n",
        "        k = self.w_k(z)\n",
        "        v = self.w_v(z)\n",
        "\n",
        "        # q, k, v를 헤드로 나눔\n",
        "        ## 먼저 벡터를 헤드 개수(h)로 나눔\n",
        "        ## (B, N, D) -> (B, N, h, D//h)\n",
        "        q = q.view(batch_size, num_patch, self.head, self.head_dim)\n",
        "        k = k.view(batch_size, num_patch, self.head, self.head_dim)\n",
        "        v = v.view(batch_size, num_patch, self.head, self.head_dim)\n",
        "\n",
        "        ## Self-Attention을 계산할 수 있게\n",
        "        ## (배치 사이즈, 헤드, 토큰 수, 패치 벡터) 형태로 변환 \n",
        "        ## (B, N, h, D//h) -> (B, h, N, D//h)\n",
        "        q = q.transpose(1,2)\n",
        "        k = k.transpose(1,2)\n",
        "        v = v.transpose(1,2)\n",
        "\n",
        "        # 내적\n",
        "        ## (B, h, N, D//h) -> (B, h, D//h, N)\n",
        "        k_T = k.transpose(2, 3)\n",
        "        ## (B, h, N, D//h) x (B, h, D//h, N) -> (B, h, N, N) \n",
        "        dots = (q @ k_T) / self.sqrt_dh\n",
        "        ## 열 방향 소프트맥스 함수\n",
        "        attn = F.softmax(dots, dim=-1)\n",
        "        ## 드롭아웃\n",
        "        attn = self.attn_drop(attn)\n",
        "        # 가중합\n",
        "        ## (B, h, N, N) x (B, h, N, D//h) -> (B, h, N, D//h) \n",
        "        out = attn @ v\n",
        "        ## (B, h, N, D//h) -> (B, N, h, D//h)\n",
        "        out = out.transpose(1, 2)\n",
        "        ## (B, N, h, D//h) -> (B, N, D)\n",
        "        out = out.reshape(batch_size, num_patch, self.emb_dim)\n",
        "\n",
        "        # 출력층\n",
        "        ## (B, N, D) -> (B, N, D) \n",
        "        out = self.w_o(out) \n",
        "        return out"
      ],
      "metadata": {
        "id": "Mzkjh1HuFAIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mhsa = MultiHeadSelfAttention()\n",
        "out = mhsa(z_0) # z_0는 z_0=input_layer(x). 사이즈는 (B, N, D)\n",
        "\n",
        "# (4, 17, 384)(=(B, N, D))가 맞는지 확인 \n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHowrSzEFWKc",
        "outputId": "2c9a9049-fc9a-4c4d-a698-0e6805a41a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 17, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VitEncoderBlock(nn.Module): \n",
        "    def __init__(self, emb_dim:int=384, head:int=4, hidden_dim:int=384*4, dropout:float=0.2):\n",
        "        \"\"\"\n",
        "        인수:\n",
        "            emb_dim: 임베딩 후 벡터 길이\n",
        "            head: 헤드 수\n",
        "            hidden_dim: Encoder Block에서 MLP 중간층의 벡터 길이 \n",
        "                        논문에서와 같이 emb_dim의 4배를 디폴트로 함\n",
        "            dropout: 드롭아웃 확률\n",
        "        \"\"\"\n",
        "        super(VitEncoderBlock, self).__init__()\n",
        "        # 첫번째 Layer Normalization\n",
        "        self.ln1 = nn.LayerNorm(emb_dim)\n",
        "        # MHSA\n",
        "        self.msa = MultiHeadSelfAttention(\n",
        "        emb_dim=emb_dim, head=head,\n",
        "        dropout = dropout,\n",
        "        )\n",
        "        # 두번째 Layer Normalization\n",
        "        self.ln2 = nn.LayerNorm(emb_dim)\n",
        "        # MLP\n",
        "        self.mlp = nn.Sequential( \n",
        "            nn.Linear(emb_dim, hidden_dim), \n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout), \n",
        "            nn.Linear(hidden_dim, emb_dim), \n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\" \n",
        "        인수:\n",
        "            z: Encoder Block으로 입력. 사이즈는 (B, N, D)\n",
        "                B: 배치 사이즈, N: 토큰 수, D: 벡터 길이\n",
        "        반환값:\n",
        "            out: Encoder Block의 출력. 사이즈는 (B, N, D)\n",
        "                B: 배치 사이즈, N: 토큰 수, D: 임베딩 벡터 길이 \n",
        "        \"\"\"\n",
        "        # Encoder Block의 전반부 \n",
        "        out = self.msa(self.ln1(z)) + z\n",
        "        # Encoder Block의 후반부 \n",
        "        out = self.mlp(self.ln2(out)) + out \n",
        "        return out"
      ],
      "metadata": {
        "id": "rx14CWY4FY4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit_enc = VitEncoderBlock()\n",
        "z_1 = vit_enc(z_0) # z_0는 z_0=input_layer(x). 사이즈는 (B, N, D)\n",
        "\n",
        "# (4, 17, 384)(=(B, N, D))가 맞는지 확인 \n",
        "print(z_1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffXfeHApFbVn",
        "outputId": "63d747b9-d256-44b2-e13d-32f06561e5c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 17, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Vit(nn.Module): \n",
        "    def __init__(self, in_channels:int=3, num_classes:int=2, emb_dim:int=384, num_patch_row:int=4, image_size:int=224,\n",
        "                 num_blocks:int=4, head:int=4, hidden_dim:int=384*4, dropout:float=0.2):\n",
        "        \"\"\" \n",
        "        인수:\n",
        "            in_channels: 입력 이미지의 채널 수\n",
        "            num_classes: 이미지 분류 클래스 수\n",
        "            emb_dim: 임베딩 후 벡터 길이\n",
        "            num_patch_row: 한 변의 패치 수\n",
        "            image_size: 입력 이미지의 한 변의 길이. 입력 이미지의 높이, 폭은 같은 길이를 가정 \n",
        "            num_blocks: Encoder Block 수\n",
        "            head: 헤드 수\n",
        "            hidden_dim: Encoder Block의 MLP 중간층의 벡터 길이 \n",
        "            dropout: 드롭아웃 확률\n",
        "        \"\"\"\n",
        "        super(Vit, self).__init__()\n",
        "        # Input Layer \n",
        "        self.input_layer = VitInputLayer(\n",
        "            in_channels, \n",
        "            emb_dim, \n",
        "            num_patch_row, \n",
        "            image_size)\n",
        "\n",
        "        # Encoder. Encoder Block 여러 층 \n",
        "        self.encoder = nn.Sequential(*[\n",
        "            VitEncoderBlock(\n",
        "                emb_dim=emb_dim,\n",
        "                head=head,\n",
        "                hidden_dim=hidden_dim,\n",
        "                dropout = dropout\n",
        "            )\n",
        "            for _ in range(num_blocks)])\n",
        "\n",
        "        # MLP Head\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(emb_dim),\n",
        "            nn.Linear(emb_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        인수:\n",
        "            x: ViT로 입력되는 이미지. 사이즈는 (B, C, H, W)\n",
        "                B: 배치 사이즈, C: 채널 수, H: 높이, W: 폭\n",
        "        반환값:\n",
        "            out: ViT의 출력. 사이즈는 (B, M)\n",
        "                B: 배치 사이즈, M: 클래스 수 \n",
        "        \"\"\"\n",
        "        # Input Layer\n",
        "        ## (B, C, H, W) -> (B, N, D)\n",
        "        ## N: 토큰 수(=배치 수+1), D: 벡터 길이 \n",
        "        out = self.input_layer(x)\n",
        "        \n",
        "        # Encoder\n",
        "        ## (B, N, D) -> (B, N, D)\n",
        "        out = self.encoder(out)\n",
        "\n",
        "        # 클래스 토큰만 꺼냄\n",
        "        ## (B, N, D) -> (B, D)\n",
        "        cls_token = out[:,0]\n",
        "\n",
        "        # MLP Head\n",
        "        ## (B, D) -> (B, M)\n",
        "        pred = self.mlp_head(cls_token)\n",
        "        return pred"
      ],
      "metadata": {
        "id": "GUgeNljGFc89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "batch_size, channel, height, width= 4, 3, 224, 224\n",
        "x = torch.randn(batch_size, channel, height, width)\n",
        "vit = Vit(in_channels=channel, num_classes=num_classes) \n",
        "pred = vit(x)\n",
        "\n",
        "# (4, 10)(=(B, M))이 맞는지 확인\n",
        "print(pred.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UOfuT3qFgFp",
        "outputId": "560a8228-febf-46cc-8292-40477236bde4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 개요 표시\n",
        "net = vit.to(device)\n",
        "summary(net,(5, 3, 224, 224))"
      ],
      "metadata": {
        "id": "o_28nNtEFiDs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1628d7ac-8204-49c5-aa37-3229fe8803ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===============================================================================================\n",
              "Layer (type:depth-idx)                        Output Shape              Param #\n",
              "===============================================================================================\n",
              "Vit                                           [5, 10]                   --\n",
              "├─VitInputLayer: 1-1                          [5, 17, 384]              6,912\n",
              "│    └─Conv2d: 2-1                            [5, 384, 4, 4]            3,613,056\n",
              "├─Sequential: 1-2                             [5, 17, 384]              --\n",
              "│    └─VitEncoderBlock: 2-2                   [5, 17, 384]              --\n",
              "│    │    └─LayerNorm: 3-1                    [5, 17, 384]              768\n",
              "│    │    └─MultiHeadSelfAttention: 3-2       [5, 17, 384]              590,208\n",
              "│    │    └─LayerNorm: 3-3                    [5, 17, 384]              768\n",
              "│    │    └─Sequential: 3-4                   [5, 17, 384]              1,181,568\n",
              "│    └─VitEncoderBlock: 2-3                   [5, 17, 384]              --\n",
              "│    │    └─LayerNorm: 3-5                    [5, 17, 384]              768\n",
              "│    │    └─MultiHeadSelfAttention: 3-6       [5, 17, 384]              590,208\n",
              "│    │    └─LayerNorm: 3-7                    [5, 17, 384]              768\n",
              "│    │    └─Sequential: 3-8                   [5, 17, 384]              1,181,568\n",
              "│    └─VitEncoderBlock: 2-4                   [5, 17, 384]              --\n",
              "│    │    └─LayerNorm: 3-9                    [5, 17, 384]              768\n",
              "│    │    └─MultiHeadSelfAttention: 3-10      [5, 17, 384]              590,208\n",
              "│    │    └─LayerNorm: 3-11                   [5, 17, 384]              768\n",
              "│    │    └─Sequential: 3-12                  [5, 17, 384]              1,181,568\n",
              "│    └─VitEncoderBlock: 2-5                   [5, 17, 384]              --\n",
              "│    │    └─LayerNorm: 3-13                   [5, 17, 384]              768\n",
              "│    │    └─MultiHeadSelfAttention: 3-14      [5, 17, 384]              590,208\n",
              "│    │    └─LayerNorm: 3-15                   [5, 17, 384]              768\n",
              "│    │    └─Sequential: 3-16                  [5, 17, 384]              1,181,568\n",
              "├─Sequential: 1-3                             [5, 10]                   --\n",
              "│    └─LayerNorm: 2-6                         [5, 384]                  768\n",
              "│    └─Linear: 2-7                            [5, 10]                   3,850\n",
              "===============================================================================================\n",
              "Total params: 10,717,834\n",
              "Trainable params: 10,717,834\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 324.53\n",
              "===============================================================================================\n",
              "Input size (MB): 3.01\n",
              "Forward/backward pass size (MB): 11.75\n",
              "Params size (MB): 42.84\n",
              "Estimated Total Size (MB): 57.61\n",
              "==============================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실 함수： 교차 엔트로피 함수\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 학습률\n",
        "lr = 0.001\n",
        "\n",
        "# 최적화 함수: 경사 하강법\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "LD-CXXzmhbKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실 계산\n",
        "loss = eval_loss(test_loader1, device, net, criterion)\n",
        "\n",
        "# 손실 계산 그래프 시각화\n",
        "g = make_dot(loss, params=dict(net.named_parameters()))\n",
        "display(g)"
      ],
      "metadata": {
        "id": "bEwbIUTBxOmB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}